{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "883c6252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2014, 200, 200, 3) (2014,)\n"
     ]
    }
   ],
   "source": [
    "# load dataset, reshape and save to a new file\n",
    "from os import listdir\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "# define location of dataset\n",
    "folder = \"C:\\\\Users\\\\nidhimh\\\\Documents\\\\Occlusion\\\\occludedornot\\\\traincropfiltered\\\\\"\n",
    "photos, labels = list(), list()\n",
    "# enumerate files in the directory\n",
    "for file in listdir(folder):\n",
    "    # determine class\n",
    "    output = 0.0\n",
    "    if file.startswith('occluded'):\n",
    "        output = 1.0\n",
    "    # load image\n",
    "    photo = load_img(folder + file, target_size=(200, 200))\n",
    "    # convert to numpy array\n",
    "    photo = img_to_array(photo)\n",
    "    # store\n",
    "    photos.append(photo)\n",
    "    labels.append(output)\n",
    "# convert to a numpy arrays\n",
    "photos = asarray(photos)\n",
    "labels = asarray(labels)\n",
    "print(photos.shape, labels.shape)\n",
    "# save the reshaped photos\n",
    "save('dataset_photos.npy', photos)\n",
    "save('dataset_labels.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37010ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize dataset into a useful structure\n",
    "from os import makedirs\n",
    "from os import listdir\n",
    "from shutil import copyfile\n",
    "from random import seed\n",
    "from random import random\n",
    "\n",
    "dataset_home = \"C:\\\\Users\\\\nidhimh\\\\Documents\\\\Occlusion\\\\occludedornot\\\\traincropfiltered\\\\\"\n",
    "subdirs = ['train/', 'test/']\n",
    "for subdir in subdirs:\n",
    "    # create label subdirectories\n",
    "    labeldirs = ['notoccluded/', 'occluded/']\n",
    "    for labldir in labeldirs:\n",
    "        newdir = dataset_home + subdir + labldir\n",
    "        makedirs(newdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "374c374b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize dataset into a useful structure\n",
    "from os import makedirs\n",
    "from os import listdir\n",
    "from shutil import copyfile\n",
    "from random import seed\n",
    "from random import random\n",
    "# seed random number generator\n",
    "seed(1)\n",
    "# define ratio of pictures to use for validation\n",
    "val_ratio = 0.25\n",
    "# copy training dataset images into subdirectories\n",
    "dataset_home = \"C:\\\\Users\\\\nidhimh\\\\Documents\\\\Occlusion\\\\occludedornot\\\\filteredrefined\\\\\"\n",
    "src_directory = dataset_home\n",
    "for file in listdir(src_directory):\n",
    "    src = src_directory + '/' + file\n",
    "    dst_dir = 'train/'\n",
    "    if random() < val_ratio:\n",
    "        dst_dir = 'test/'\n",
    "    if file.startswith('occluded'):\n",
    "        dst = dataset_home + dst_dir + 'occluded/'  + file\n",
    "        copyfile(src, dst)\n",
    "    elif file.startswith('notoccluded'):\n",
    "        dst = dataset_home + dst_dir + 'notoccluded/'  + file\n",
    "        copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7f9d243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1448 images belonging to 2 classes.\n",
      "Found 459 images belonging to 2 classes.\n",
      "Epoch 1/25\n",
      "23/23 [==============================] - 82s 4s/step - loss: 0.7991 - acc: 0.6009 - val_loss: 0.6688 - val_acc: 0.6122\n",
      "Epoch 2/25\n",
      "23/23 [==============================] - 79s 3s/step - loss: 0.6253 - acc: 0.6714 - val_loss: 0.6593 - val_acc: 0.6340\n",
      "Epoch 3/25\n",
      "23/23 [==============================] - 78s 3s/step - loss: 0.6331 - acc: 0.6591 - val_loss: 0.6592 - val_acc: 0.6362\n",
      "Epoch 4/25\n",
      "23/23 [==============================] - 77s 3s/step - loss: 0.6209 - acc: 0.6780 - val_loss: 0.6562 - val_acc: 0.6601\n",
      "Epoch 5/25\n",
      "23/23 [==============================] - 77s 3s/step - loss: 0.6150 - acc: 0.6852 - val_loss: 0.6508 - val_acc: 0.6514\n",
      "Epoch 6/25\n",
      "23/23 [==============================] - 78s 3s/step - loss: 0.6134 - acc: 0.6913 - val_loss: 0.6478 - val_acc: 0.6754\n",
      "Epoch 7/25\n",
      "23/23 [==============================] - 77s 3s/step - loss: 0.6106 - acc: 0.6833 - val_loss: 0.6447 - val_acc: 0.6645\n",
      "Epoch 8/25\n",
      "23/23 [==============================] - 79s 3s/step - loss: 0.5949 - acc: 0.7050 - val_loss: 0.6375 - val_acc: 0.6776\n",
      "Epoch 9/25\n",
      "23/23 [==============================] - 78s 3s/step - loss: 0.5941 - acc: 0.6951 - val_loss: 0.6362 - val_acc: 0.6667\n",
      "Epoch 10/25\n",
      "23/23 [==============================] - 78s 3s/step - loss: 0.6035 - acc: 0.6900 - val_loss: 0.6317 - val_acc: 0.6841\n",
      "Epoch 11/25\n",
      "23/23 [==============================] - 78s 3s/step - loss: 0.5816 - acc: 0.7160 - val_loss: 0.6208 - val_acc: 0.6776\n",
      "Epoch 12/25\n",
      "23/23 [==============================] - 78s 3s/step - loss: 0.5714 - acc: 0.7089 - val_loss: 0.6128 - val_acc: 0.6819\n",
      "Epoch 13/25\n",
      "23/23 [==============================] - 78s 3s/step - loss: 0.5720 - acc: 0.7273 - val_loss: 0.6049 - val_acc: 0.7015\n",
      "Epoch 14/25\n",
      "23/23 [==============================] - 78s 3s/step - loss: 0.5824 - acc: 0.7186 - val_loss: 0.6208 - val_acc: 0.6645\n",
      "Epoch 15/25\n",
      "23/23 [==============================] - 77s 3s/step - loss: 0.5681 - acc: 0.7235 - val_loss: 0.6060 - val_acc: 0.6972\n",
      "Epoch 16/25\n",
      "23/23 [==============================] - 82s 4s/step - loss: 0.5754 - acc: 0.7180 - val_loss: 0.6017 - val_acc: 0.6885\n",
      "Epoch 17/25\n",
      "23/23 [==============================] - 82s 4s/step - loss: 0.5614 - acc: 0.7279 - val_loss: 0.6052 - val_acc: 0.6885\n",
      "Epoch 18/25\n",
      "23/23 [==============================] - 82s 4s/step - loss: 0.5572 - acc: 0.7387 - val_loss: 0.6009 - val_acc: 0.7081\n",
      "Epoch 19/25\n",
      "23/23 [==============================] - 84s 4s/step - loss: 0.5505 - acc: 0.7295 - val_loss: 0.5947 - val_acc: 0.7233\n",
      "Epoch 20/25\n",
      "23/23 [==============================] - 79s 3s/step - loss: 0.5548 - acc: 0.7298 - val_loss: 0.5939 - val_acc: 0.6906\n",
      "Epoch 21/25\n",
      "23/23 [==============================] - 83s 4s/step - loss: 0.5405 - acc: 0.7422 - val_loss: 0.5846 - val_acc: 0.7124\n",
      "Epoch 22/25\n",
      "23/23 [==============================] - 81s 4s/step - loss: 0.5363 - acc: 0.7406 - val_loss: 0.5867 - val_acc: 0.7102\n",
      "Epoch 23/25\n",
      "23/23 [==============================] - 82s 4s/step - loss: 0.5464 - acc: 0.7407 - val_loss: 0.5772 - val_acc: 0.7124\n",
      "Epoch 24/25\n",
      "23/23 [==============================] - 82s 4s/step - loss: 0.5409 - acc: 0.7253 - val_loss: 0.5837 - val_acc: 0.7081\n",
      "Epoch 25/25\n",
      "23/23 [==============================] - 117s 5s/step - loss: 0.5437 - acc: 0.7398 - val_loss: 0.5762 - val_acc: 0.7168\n",
      "> 71.678\n"
     ]
    }
   ],
   "source": [
    "# baseline model for the dataset\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(200, 200, 3)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(200, 200, 3)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(200, 200, 3)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    opt = SGD(lr=0.001, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(history):\n",
    "    # plot loss\n",
    "    pyplot.subplot(211)\n",
    "    pyplot.title('Cross Entropy Loss')\n",
    "    pyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "    pyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    "    # plot accuracy\n",
    "    pyplot.subplot(212)\n",
    "    pyplot.title('Classification Accuracy')\n",
    "    pyplot.plot(history.history['acc'], color='blue', label='train')\n",
    "    pyplot.plot(history.history['val_acc'], color='orange', label='test')\n",
    "    # save plot to file\n",
    "    filename = sys.argv[0].split('/')[-1]\n",
    "    pyplot.savefig(filename + '_plot.png')\n",
    "    pyplot.close()\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "    # define model\n",
    "    model = define_model()\n",
    "    # create data generator\n",
    "    datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    # prepare iterators\n",
    "    train_it = datagen.flow_from_directory('C:\\\\Users\\\\nidhimh\\\\Documents\\\\Occlusion\\\\occludedornot\\\\filteredrefined\\\\train\\\\',\n",
    "        class_mode='binary', batch_size=64, target_size=(200, 200))\n",
    "    test_it = datagen.flow_from_directory('C:\\\\Users\\\\nidhimh\\\\Documents\\\\Occlusion\\\\occludedornot\\\\filteredrefined\\\\test\\\\',\n",
    "        class_mode='binary', batch_size=64, target_size=(200, 200))\n",
    "    # fit model\n",
    "    history = model.fit_generator(train_it, steps_per_epoch=len(train_it),\n",
    "        validation_data=test_it, validation_steps=len(test_it), epochs=25)\n",
    "    # save model \n",
    "    model.save('occlusionstate.h5')\n",
    "    model.save_weights('occlusion_weights.h5')\n",
    "    # evaluate model\n",
    "    _, acc = model.evaluate_generator(test_it, steps=len(test_it))\n",
    "    print('> %.3f' % (acc * 100.0))\n",
    "    # learning curves\n",
    "    summarize_diagnostics(history)\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1120982c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
